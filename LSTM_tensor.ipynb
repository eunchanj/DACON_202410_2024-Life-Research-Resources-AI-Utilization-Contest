{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890a6164-b8c3-4924-8d7e-339ed3cb8b10",
   "metadata": {},
   "source": [
    "# 241021\n",
    "- Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78b3d3b-b9db-4fd1-87c9-45c90d75d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:14:09.513006: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-21 01:14:09.530627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-21 01:14:09.549772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-21 01:14:09.555489: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-21 01:14:09.570195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-21 01:14:10.770822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package wordnet to /home/evidnet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f152b6-faa7-430e-9053-28f8f7a4e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 분포:\n",
      "SUBCLASS_encoded\n",
      "2     786\n",
      "8     515\n",
      "6     461\n",
      "21    379\n",
      "9     334\n",
      "23    324\n",
      "20    276\n",
      "18    266\n",
      "15    253\n",
      "11    229\n",
      "7     223\n",
      "4     223\n",
      "25    198\n",
      "19    198\n",
      "13    184\n",
      "14    178\n",
      "12    158\n",
      "10    158\n",
      "3     155\n",
      "17    147\n",
      "22    124\n",
      "16    120\n",
      "1     104\n",
      "24     98\n",
      "0      72\n",
      "5      38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "train.fillna('WT', inplace=True)\n",
    "test.fillna('WT', inplace=True)\n",
    "\n",
    "mutation_columns = [col for col in train.columns if col not in ['ID', 'SUBCLASS']]\n",
    "\n",
    "train['mutations'] = train[mutation_columns].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "test['mutations'] = test[mutation_columns].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['SUBCLASS_encoded'] = label_encoder.fit_transform(train['SUBCLASS'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "X = train['mutations']\n",
    "y = train['SUBCLASS_encoded']\n",
    "\n",
    "# 클래스 분포 확인\n",
    "class_counts = train['SUBCLASS_encoded'].value_counts()\n",
    "print(\"클래스 분포:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdf9831-f7cd-4508-ad08-81f3e260f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "증강 후 클래스 분포:\n",
      "SUBCLASS_encoded\n",
      "8     786\n",
      "19    786\n",
      "20    786\n",
      "9     786\n",
      "6     786\n",
      "21    786\n",
      "2     786\n",
      "23    786\n",
      "12    786\n",
      "7     786\n",
      "16    786\n",
      "15    786\n",
      "18    786\n",
      "25    786\n",
      "10    786\n",
      "4     786\n",
      "0     786\n",
      "11    786\n",
      "14    786\n",
      "13    786\n",
      "3     786\n",
      "17    786\n",
      "24    786\n",
      "1     786\n",
      "22    786\n",
      "5     786\n",
      "Name: count, dtype: int64\n",
      "최대 시퀀스 길이: 12355\n"
     ]
    }
   ],
   "source": [
    "# 2. 데이터 증강 기법 정의 (EDA)\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word != 'WT']))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return new_words\n",
    "\n",
    "def random_insertion(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1 and counter < 10:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        counter +=1\n",
    "    if synonyms:\n",
    "        synonym = synonyms[0].lemmas()[0].name()\n",
    "        random_idx = random.randint(0, len(new_words)-1)\n",
    "        new_words.insert(random_idx, synonym)\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1 = random.randint(0, len(new_words)-1)\n",
    "        idx2 = random.randint(0, len(new_words)-1)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "    return new_words\n",
    "\n",
    "def random_deletion(words, p):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    new_words = [word for word in words if random.uniform(0,1) > p]\n",
    "    if len(new_words) == 0:\n",
    "        return [words[random.randint(0,len(words)-1)]]\n",
    "    return new_words\n",
    "\n",
    "def eda(sentence, num_aug=4):\n",
    "    words = sentence.split()\n",
    "    num_words = len(words)\n",
    "    augmented_sentences = []\n",
    "    n_sr = max(1, int(0.1*num_words))\n",
    "    n_ri = max(1, int(0.1*num_words))\n",
    "    n_rs = max(1, int(0.1*num_words))\n",
    "    # 동의어 교체\n",
    "    a_words = synonym_replacement(words, n_sr)\n",
    "    augmented_sentences.append(' '.join(a_words))\n",
    "    # 무작위 삽입\n",
    "    a_words = random_insertion(words, n_ri)\n",
    "    augmented_sentences.append(' '.join(a_words))\n",
    "    # 무작위 교환\n",
    "    a_words = random_swap(words, n_rs)\n",
    "    augmented_sentences.append(' '.join(a_words))\n",
    "    # 무작위 삭제\n",
    "    a_words = random_deletion(words, p=0.1)\n",
    "    augmented_sentences.append(' '.join(a_words))\n",
    "    return augmented_sentences\n",
    "\n",
    "# 3. 소수 클래스에 대한 데이터 증강\n",
    "max_class_count = class_counts.max()\n",
    "\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for class_label in class_counts.index:\n",
    "    count = class_counts[class_label]\n",
    "    if count < max_class_count:\n",
    "        df_class = train[train['SUBCLASS_encoded'] == class_label]\n",
    "        texts = df_class['mutations'].tolist()\n",
    "        augment_count = max_class_count - count\n",
    "        i = 0\n",
    "        while augment_count > 0:\n",
    "            text = texts[i % len(texts)]\n",
    "            aug_texts = eda(text)\n",
    "            for aug_text in aug_texts:\n",
    "                augmented_texts.append(aug_text)\n",
    "                augmented_labels.append(class_label)\n",
    "                augment_count -= 1\n",
    "                if augment_count == 0:\n",
    "                    break\n",
    "            i += 1\n",
    "\n",
    "# 증강된 데이터프레임 생성\n",
    "augmented_df = pd.DataFrame({'mutations': augmented_texts, 'SUBCLASS_encoded': augmented_labels})\n",
    "\n",
    "# 원본 데이터와 증강된 데이터 결합\n",
    "train_augmented = pd.concat([train[['mutations', 'SUBCLASS_encoded']], augmented_df], ignore_index=True)\n",
    "\n",
    "# 클래스 분포 재확인\n",
    "print(\"증강 후 클래스 분포:\")\n",
    "print(train_augmented['SUBCLASS_encoded'].value_counts())\n",
    "\n",
    "# 4. 토크나이저 및 시퀀스 변환\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_augmented['mutations'])\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(train_augmented['mutations'])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test['mutations'])\n",
    "\n",
    "# 시퀀스 패딩\n",
    "max_seq_length = max(len(seq) for seq in X_sequences)\n",
    "print(f\"최대 시퀀스 길이: {max_seq_length}\")\n",
    "\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_seq_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# 타겟 레이블\n",
    "y = train_augmented['SUBCLASS_encoded']\n",
    "\n",
    "# 5. 훈련 세트와 검증 세트로 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_padded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22034c21-a120-43f5-b453-70c1cde6c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-10-21 01:19:06.184542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22990 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2024-10-21 01:19:06.185223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22982 MB memory:  -> device: 1, name: NVIDIA TITAN RTX, pci bus id: 0000:73:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:19:15.127083: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 1s/step - accuracy: 0.0375 - loss: 3.2618 - val_accuracy: 0.0416 - val_loss: 3.2573\n",
      "Epoch 2/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 2s/step - accuracy: 0.0401 - loss: 3.2581 - val_accuracy: 0.0409 - val_loss: 3.2631\n",
      "Epoch 3/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 2s/step - accuracy: 0.0423 - loss: 3.2621 - val_accuracy: 0.0634 - val_loss: 3.1774\n",
      "Epoch 4/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 2s/step - accuracy: 0.0886 - loss: 3.0273 - val_accuracy: 0.1959 - val_loss: 2.4560\n",
      "Epoch 5/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 2s/step - accuracy: 0.1993 - loss: 2.3703 - val_accuracy: 0.3694 - val_loss: 1.9542\n",
      "Epoch 6/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 2s/step - accuracy: 0.3485 - loss: 1.8551 - val_accuracy: 0.2546 - val_loss: 2.1472\n",
      "Epoch 7/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 2s/step - accuracy: 0.3639 - loss: 1.7760 - val_accuracy: 0.4724 - val_loss: 1.6061\n",
      "Epoch 8/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 2s/step - accuracy: 0.5376 - loss: 1.3013 - val_accuracy: 0.5482 - val_loss: 1.5418\n",
      "Epoch 9/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 2s/step - accuracy: 0.6234 - loss: 1.0823 - val_accuracy: 0.7187 - val_loss: 1.1488\n",
      "Epoch 10/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 2s/step - accuracy: 0.7236 - loss: 0.8713 - val_accuracy: 0.7336 - val_loss: 1.0978\n",
      "Epoch 11/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 2s/step - accuracy: 0.7919 - loss: 0.7050 - val_accuracy: 0.7894 - val_loss: 1.0013\n",
      "Epoch 12/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 2s/step - accuracy: 0.7967 - loss: 0.7092 - val_accuracy: 0.7302 - val_loss: 1.0457\n",
      "Epoch 13/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 2s/step - accuracy: 0.7647 - loss: 0.7624 - val_accuracy: 0.7710 - val_loss: 0.9850\n",
      "Epoch 14/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 2s/step - accuracy: 0.8352 - loss: 0.5773 - val_accuracy: 0.7972 - val_loss: 0.9183\n",
      "Epoch 15/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 2s/step - accuracy: 0.8632 - loss: 0.4970 - val_accuracy: 0.7735 - val_loss: 0.9996\n",
      "Epoch 16/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 2s/step - accuracy: 0.8155 - loss: 0.5900 - val_accuracy: 0.8160 - val_loss: 0.9236\n",
      "Epoch 17/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 2s/step - accuracy: 0.8723 - loss: 0.4515 - val_accuracy: 0.8053 - val_loss: 0.9629\n",
      "Epoch 18/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 2s/step - accuracy: 0.8784 - loss: 0.4371 - val_accuracy: 0.8246 - val_loss: 0.9299\n",
      "Epoch 19/50\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 2s/step - accuracy: 0.8893 - loss: 0.4054 - val_accuracy: 0.8275 - val_loss: 0.9478\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 513ms/step\n",
      "검증 데이터 성능 평가:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACC       0.99      0.90      0.94       157\n",
      "        BLCA       0.96      0.99      0.98       157\n",
      "        BRCA       0.38      0.41      0.40       157\n",
      "        CESC       0.91      0.92      0.91       157\n",
      "        COAD       0.90      0.87      0.89       157\n",
      "        DLBC       0.99      0.99      0.99       157\n",
      "      GBMLGG       0.69      0.37      0.48       158\n",
      "        HNSC       0.82      0.82      0.82       158\n",
      "       KIPAN       0.57      0.22      0.31       157\n",
      "        KIRC       0.27      0.68      0.39       157\n",
      "        LAML       0.59      0.94      0.73       158\n",
      "         LGG       0.70      0.82      0.76       157\n",
      "        LIHC       0.90      0.96      0.93       157\n",
      "        LUAD       0.96      0.92      0.94       157\n",
      "        LUSC       0.99      0.92      0.96       158\n",
      "          OV       0.80      0.85      0.82       157\n",
      "        PAAD       0.97      0.97      0.97       157\n",
      "        PCPG       0.85      0.96      0.90       157\n",
      "        PRAD       0.82      0.85      0.84       157\n",
      "        SARC       0.89      0.67      0.77       158\n",
      "        SKCM       0.99      0.77      0.86       158\n",
      "        STES       0.88      0.66      0.75       157\n",
      "        TGCT       1.00      0.98      0.99       157\n",
      "        THCA       0.95      0.66      0.78       157\n",
      "        THYM       0.95      0.69      0.80       157\n",
      "        UCEC       0.95      0.94      0.95       157\n",
      "\n",
      "    accuracy                           0.80      4088\n",
      "   macro avg       0.83      0.80      0.80      4088\n",
      "weighted avg       0.83      0.80      0.80      4088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. 딥러닝 모델 정의\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_length))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 7. 모델 컴파일\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 8. 모델 학습\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 9. 모델 평가\n",
    "y_val_pred_probs = model.predict(X_val)\n",
    "y_val_pred = np.argmax(y_val_pred_probs, axis=1)\n",
    "\n",
    "print(\"검증 데이터 성능 평가:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fc2949-5caa-4a79-8c8d-172148ea6fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 504ms/step\n",
      "제출 파일이 생성되었습니다: submission_LSTM_augmented.csv\n"
     ]
    }
   ],
   "source": [
    "# 10. 테스트 데이터 예측 및 제출 파일 생성\n",
    "y_test_pred_probs = model.predict(X_test_padded)\n",
    "y_test_pred = np.argmax(y_test_pred_probs, axis=1)\n",
    "\n",
    "test['SUBCLASS'] = label_encoder.inverse_transform(y_test_pred)\n",
    "\n",
    "submission = test[['ID', 'SUBCLASS']]\n",
    "submission.to_csv('submission_LSTM_augmented.csv', index=False)\n",
    "print(\"제출 파일이 생성되었습니다: submission_LSTM_augmented.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
