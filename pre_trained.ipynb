{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f4f97e-2c7e-4345-8f19-a635cccb18ad",
   "metadata": {},
   "source": [
    "# 1st\n",
    "- pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaae771-cd32-4e08-99c5-bf4d047f7c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0511d8374043d0adaee5d08f8dad59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88478b913788442e97ad7458920091a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4239e2aac93f42e6bc68e451a6f2c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6973f07d7530480087dcc842d779f0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1051fc864b4128bfdba97f9d5c30a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Preprocess data\n",
    "X = train_data.drop(columns=['SUBCLASS', 'ID'], errors='ignore')\n",
    "y = train_data['SUBCLASS']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# List of pre trained models\n",
    "pre_trained_model1 = 'bert-base-uncased'\n",
    "pre_trained_model2 = 'zhihan1996/DNA_bert_6'\n",
    "\n",
    "# Define custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, tokenizer_name='bert-base-uncased', max_length=128):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx].astype(str).values\n",
    "        text = ' '.join(text)\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return input_ids, attention_mask, label\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Data augmentation function\n",
    "def augment_text(text, n_augments=3):\n",
    "    words = text.split()\n",
    "    augmented_texts = []\n",
    "    for _ in range(n_augments):\n",
    "        random.shuffle(words)\n",
    "        augmented_texts.append(' '.join(words))\n",
    "    return augmented_texts\n",
    "\n",
    "# Apply data augmentation on training data\n",
    "augmented_data = []\n",
    "augmented_labels = []\n",
    "for idx in range(len(X_train)):\n",
    "    text = ' '.join(X_train.iloc[idx].astype(str).values)\n",
    "    aug_texts = augment_text(text)\n",
    "    for aug_text in aug_texts:\n",
    "        augmented_data.append(aug_text)\n",
    "        augmented_labels.append(y_train[idx])\n",
    "\n",
    "augmented_df = pd.DataFrame({'text': augmented_data})\n",
    "X_train_augmented = pd.concat([X_train, augmented_df], ignore_index=True)\n",
    "y_train_augmented = np.concatenate([y_train, augmented_labels])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(X_train_augmented, y_train_augmented)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(test_data.drop(columns=['ID'], errors='ignore'))\n",
    "\n",
    "batch_size = 8 if torch.cuda.is_available() else 4\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * 3\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e088f2-ddb2-436f-8cbd-41a806377d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 3.0978\n",
      "Epoch 2/20, Training Loss: 3.0709\n",
      "Epoch 3/20, Training Loss: 3.0546\n",
      "Epoch 4/20, Training Loss: 3.0489\n",
      "Epoch 5/20, Training Loss: 3.0483\n",
      "Epoch 6/20, Training Loss: 3.0487\n",
      "Epoch 7/20, Training Loss: 3.0490\n",
      "Epoch 8/20, Training Loss: 3.0479\n",
      "Epoch 9/20, Training Loss: 3.0485\n",
      "Epoch 10/20, Training Loss: 3.0495\n",
      "Epoch 11/20, Training Loss: 3.0484\n",
      "Epoch 12/20, Training Loss: 3.0494\n",
      "Epoch 13/20, Training Loss: 3.0493\n",
      "Epoch 14/20, Training Loss: 3.0495\n",
      "Epoch 15/20, Training Loss: 3.0479\n",
      "Epoch 16/20, Training Loss: 3.0477\n",
      "Epoch 17/20, Training Loss: 3.0489\n",
      "Epoch 18/20, Training Loss: 3.0485\n",
      "Epoch 19/20, Training Loss: 3.0480\n",
      "Epoch 20/20, Training Loss: 3.0473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACC       0.00      0.00      0.00        14\n",
      "        BLCA       0.00      0.00      0.00        21\n",
      "        BRCA       0.14      0.90      0.24       157\n",
      "        CESC       0.00      0.00      0.00        31\n",
      "        COAD       0.00      0.00      0.00        45\n",
      "        DLBC       0.00      0.00      0.00         7\n",
      "      GBMLGG       0.00      0.00      0.00        92\n",
      "        HNSC       0.00      0.00      0.00        45\n",
      "       KIPAN       0.00      0.00      0.00       103\n",
      "        KIRC       0.00      0.00      0.00        67\n",
      "        LAML       0.00      0.00      0.00        32\n",
      "         LGG       0.00      0.00      0.00        46\n",
      "        LIHC       0.00      0.00      0.00        31\n",
      "        LUAD       0.00      0.00      0.00        37\n",
      "        LUSC       0.00      0.00      0.00        36\n",
      "          OV       0.00      0.00      0.00        51\n",
      "        PAAD       0.00      0.00      0.00        24\n",
      "        PCPG       0.00      0.00      0.00        29\n",
      "        PRAD       0.00      0.00      0.00        53\n",
      "        SARC       0.00      0.00      0.00        40\n",
      "        SKCM       0.20      0.53      0.29        55\n",
      "        STES       0.11      0.14      0.13        76\n",
      "        TGCT       0.00      0.00      0.00        25\n",
      "        THCA       0.00      0.00      0.00        65\n",
      "        THYM       0.00      0.00      0.00        19\n",
      "        UCEC       0.00      0.00      0.00        40\n",
      "\n",
      "    accuracy                           0.15      1241\n",
      "   macro avg       0.02      0.06      0.03      1241\n",
      "weighted avg       0.03      0.15      0.05      1241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = tuple(b.to(device) for b in batch)\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "y_val_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = tuple(b.to(device) for b in batch)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        y_val_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589b2b02-38c4-4c1c-a277-95e9ee95cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set prediction\n",
    "model.eval()\n",
    "y_test_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask = tuple(b.to(device) for b in batch)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        y_test_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({'ID': test_data['ID'], 'SUBCLASS': label_encoder.inverse_transform(y_test_pred)})\n",
    "submission.to_csv('data/submission/final_1st.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
